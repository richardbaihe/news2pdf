{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e5ac55eb5caa09446d7f66b2982ae434449aebf62580d30fd42992979cd80d1a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pypandoc\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_bs4(url):\n",
    "       hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "       request = urllib.request.Request(url, headers=hdr)\n",
    "       data_str = urllib.request.urlopen(request, timeout=10).read()\n",
    "       data_str = data_str.decode('utf-8')\n",
    "       parsed_html = BeautifulSoup(data_str,'html.parser')\n",
    "       return parsed_html\n",
    "\n",
    "def get_url_list(archive_url,cover=False):\n",
    "       parsed_html = url_to_bs4(archive_url)\n",
    "       json_dict = json.loads(parsed_html.find('script',attrs={'type':'application/json'}).string)\n",
    "       all_items = json_dict['props']['pageProps']['content']['hasPart']['parts']\n",
    "       edition_url_list = [item['url']['canonical'] for item in all_items]\n",
    "       if cover:\n",
    "              cover_url_list = [item['image']['cover'][0]['url']['canonical'] for item in all_items]\n",
    "              return edition_url_list,cover_url_list\n",
    "       else:\n",
    "              return edition_url_list\n",
    "\n",
    "def html_text_piece_to_latex(item):\n",
    "    if 'class' not in item.attrs:\n",
    "        return None\n",
    "    item_class = item.attrs['class']\n",
    "    if 'article__body-text' not in item_class:\n",
    "        return None\n",
    "    paragraph = pypandoc.convert_text(item, 'tex', format='html').replace('\\n',' ')\n",
    "    if 'article__body-text--dropcap' in item_class:\n",
    "        paragraph = '\\lettrine'+paragraph\n",
    "    return paragraph\n",
    "\n",
    "\n",
    "def convert_image_url_to_tex(image_url,tex_path, is_main_image=True):\n",
    "        \n",
    "        image_local_path = os.path.join(tex_path,'images',image_url.split('/')[-1])\n",
    "        os.system(\"wget -O \"+image_local_path+\" \"+image_url)\n",
    "        if is_main_image:\n",
    "            image_tex = \"\\\\begin{figure*}[h]\\n\\\\centering\\n\\\\includegraphics[width=0.8\\\\textwidth]{images/%s}\\n\\\\end{figure*}\\n\" % image_url.split('/')[-1]\n",
    "        else:\n",
    "            #image_tex = \"\\\\begin{wrapfigure}[20]{L}{0.5\\textwidth}\\n\\\\includegraphics[width=0.4\\\\textwidth]{images/%s}\\n\\\\end{wrapfigure}\\n\" % image_url.split('/')[-1]\n",
    "            image_tex = \"\\\\begin{figure*}[h]\\n\\\\centering\\n\\\\includegraphics[width=0.4\\\\textwidth]{images/%s}\\n\\\\end{figure*}\\n\" % image_url.split('/')[-1]\n",
    "        return image_tex\n",
    "\n",
    "\n",
    "def convert_article_ulr_to_latex(article_url,tex_path):\n",
    "    parsed_article = url_to_bs4(article_url)\n",
    "    # meta info\n",
    "    json_dict = json.loads(parsed_article.find(\"script\",attrs={'type':\"application/json\"}).string)['props']['pageProps']['content']\n",
    "    if type(json_dict) is list:\n",
    "        json_dict = json_dict[0]\n",
    "    meta_dict_str = {'article_headline':json_dict['headline'],'article_subheadline':json_dict['subheadline'],'description':json_dict['description'], 'section':json_dict['_section']['sectionHeadline'], 'subsection':json_dict['_section']['sectionSubheadline'], \"date\":json_dict['datePublishedString']}\n",
    "    meta_dict_tex = {k: pypandoc.convert_text(v, 'tex', format='html').replace('\\n',' ') for k,v in meta_dict_str.items()}\n",
    "    # main image\n",
    "    main_image_url = json_dict['image']['main']['url']['canonical']\n",
    "    main_image_tex = convert_image_url_to_tex(main_image_url,tex_path)\n",
    "    # article body\n",
    "    article = []\n",
    "    for item in parsed_article.find('div',attrs={'class':'ds-layout-grid ds-layout-grid--edged layout-article-body'}).contents:\n",
    "        if 'class' not in item.attrs:\n",
    "            continue\n",
    "        item_class = item.attrs['class']\n",
    "        if 'article__body-text-image' in item_class:\n",
    "            body_image_url = item.find('div',attrs={'itemprop':'image'}).find(\"meta\",attrs={\"itemprop\":\"url\"})['content']  \n",
    "            article.append(convert_image_url_to_tex(body_image_url,tex_path,is_main_image=False))\n",
    "            for sub_item in item.contents:\n",
    "                paragraph = html_text_piece_to_latex(sub_item)\n",
    "                if paragraph:\n",
    "                    article.append(paragraph)\n",
    "        else:\n",
    "            paragraph = html_text_piece_to_latex(item)\n",
    "            if paragraph:\n",
    "                    article.append(paragraph)\n",
    "    final_tex = main_image_tex + '\\n\\n'.join(article)\n",
    "    final_tex = final_tex.replace('■','').replace(\"\\\\euro{}\",\"€\")\n",
    "    return final_tex, meta_dict_tex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-03-27\n"
     ]
    }
   ],
   "source": [
    "archive_url = 'https://www.economist.com/weeklyedition/archive'\n",
    "edition_url_list,cover_image_url_list = get_url_list(archive_url,cover=True)\n",
    "lastest_edition_url = edition_url_list[0]\n",
    "latest_cover_url = cover_image_url_list[0]\n",
    "edition_name = lastest_edition_url.split('/')[-1]\n",
    "tex_path = os.path.join('tex',edition_name)\n",
    "all_articles_url = get_url_list(lastest_edition_url)\n",
    "tex_prefix = open('template.tex','r').read()\n",
    "print(edition_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 82/82 [02:41<00:00,  1.97s/it]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "pathlib.Path(tex_path+'/images').mkdir(parents=True, exist_ok=True)\n",
    "image_local_path = os.path.join(tex_path,'images','cover.jpg')\n",
    "os.system(\"wget -O \"+image_local_path+\" \"+latest_cover_url)\n",
    "with open(os.path.join(tex_path,'main.tex'),'w', encoding='utf-8') as f_open:\n",
    "    f_open.write(tex_prefix)\n",
    "    current_section = ''\n",
    "    for article_url in tqdm(all_articles_url):\n",
    "        try:\n",
    "            tex, meta_info = convert_article_ulr_to_latex(article_url,tex_path)\n",
    "        except:\n",
    "            continue\n",
    "        if not current_section:\n",
    "            current_section = meta_info['section']\n",
    "            f_open.write (\"\\\\section{%s}\\n\"%meta_info['section'])\n",
    "        elif current_section!=meta_info['section']:\n",
    "            f_open.write (\"\\\\section{%s}\\n\"%meta_info['section'])\n",
    "            current_section = meta_info['section']\n",
    "        f_open.write (\"\\\\subsubsection{%s}\\n\"%meta_info['article_subheadline'])\n",
    "        f_open.write (\"\\\\subsection{%s}\\n\"%meta_info['article_headline'])\n",
    "        f_open.write (\"\\\\paragraph{Print Edition | %s \\quad \\color{gray}{%s}}\\n\"%(meta_info['section'] ,meta_info['date']))\n",
    "\n",
    "        f_open.write (tex+'\\n')\n",
    "        f_open.write('\\\\clearpage\\n')\n",
    "    f_open.write ('\\\\end{document}')\n",
    "working_directory = tex_path\n",
    "p = subprocess.Popen(['latexmk', '-pdflua', 'main.tex'], cwd=working_directory)\n",
    "p.wait()\n",
    "p = subprocess.Popen(['latexmk', '-c'], cwd=working_directory)\n",
    "p.wait()\n",
    "p = subprocess.Popen(['mv',os.path.join(tex_path,'main.pdf'),'./'+ edition_name+'.pdf'])\n",
    "p.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen(['rm','-rf',tex_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}